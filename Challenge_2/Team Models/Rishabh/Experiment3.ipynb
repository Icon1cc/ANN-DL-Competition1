{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Dropout, Bidirectional, BatchNormalization\n",
    "\n",
    "# Reproducibility\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# Load your data\n",
    "categories_path = '/Users/rishabhtiwari/Desktop/training_dataset/categories.npy'\n",
    "valid_periods_path = '/Users/rishabhtiwari/Desktop/training_dataset/valid_periods.npy'\n",
    "training_data_path = '/Users/rishabhtiwari/Desktop/training_dataset/training_data.npy'\n",
    "\n",
    "categories = np.load(categories_path)\n",
    "valid_periods = np.load(valid_periods_path)\n",
    "training_data = np.load(training_data_path)\n",
    "\n",
    "# Constants for sequence and forecast lengths\n",
    "seq_length = 128     # predictions based on previous seq_length data entries\n",
    "forecast_length = 9  # predicting forecast_length time steps into the future\n",
    "\n",
    "# Data preprocessing\n",
    "def preprocess_data(data, valid_periods, scaler):\n",
    "    preprocessed_data = []\n",
    "    for i, row in enumerate(data):\n",
    "        valid_data = row[valid_periods[i][0]:valid_periods[i][1]]\n",
    "        if valid_data.size > 0:\n",
    "            scaled_data = scaler.fit_transform(valid_data.reshape(-1, 1)).flatten()\n",
    "            preprocessed_data.append(scaled_data)\n",
    "    return preprocessed_data\n",
    "\n",
    "# Convert the time series data into sequences\n",
    "def to_sequences(data):\n",
    "    X, y = [], []\n",
    "    for ts in data:\n",
    "        for i in range(len(ts) - seq_length - forecast_length + 1):\n",
    "            X.append(ts[i:(i + seq_length)])\n",
    "            y.append(ts[(i + seq_length):(i + seq_length + forecast_length)])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Apply preprocessing\n",
    "scaler = StandardScaler()\n",
    "data = preprocess_data(training_data, valid_periods, scaler)\n",
    "X, y = to_sequences(data)\n",
    "\n",
    "# Data splitting\n",
    "val_size = int(len(X) * 0.2)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X_train, y_train = X[indices[:-val_size]], y[indices[:-val_size]]\n",
    "X_val, y_val = X[indices[-val_size:]], y[indices[-val_size:]]\n",
    "\n",
    "# Reshape data for LSTM input\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "y_val = y_val.reshape((y_val.shape[0], y_val.shape[1], 1))\n",
    "\n",
    "# Model building\n",
    "def build_advanced_model(input_shape, rnn_units, dropout_rate, forecast_length, use_gru=True):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    if use_gru:\n",
    "        x = Bidirectional(GRU(rnn_units, return_sequences=True))(input_layer)\n",
    "    else:\n",
    "        x = Bidirectional(LSTM(rnn_units, return_sequences=True))(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    if use_gru:\n",
    "        x = GRU(rnn_units // 2)(x)\n",
    "    else:\n",
    "        x = LSTM(rnn_units // 2)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    output_layer = Dense(forecast_length)(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "dropout_rate = 0.2\n",
    "rnn_units = 128\n",
    "forecast_length = y_train.shape[1]  # Assumes y_train is already shaped as needed\n",
    "\n",
    "model = build_advanced_model(input_shape, rnn_units, dropout_rate, forecast_length, use_gru=True)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Model training\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_val, y_val), callbacks=[early_stopping], verbose=1)\n",
    "\n",
    "model.save('SubmissionModel')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
