{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from joblib import dump\n",
    "\n",
    "# Set up for reproducible results\n",
    "seed = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the .npy files\n",
    "categories_path = '/Users/rishabhtiwari/Desktop/training_dataset/categories.npy'\n",
    "valid_periods_path = '/Users/rishabhtiwari/Desktop/training_dataset/valid_periods.npy'\n",
    "training_data_path = '/Users/rishabhtiwari/Desktop/training_dataset/training_data.npy'\n",
    "\n",
    "# Load the data\n",
    "categories = np.load(categories_path)\n",
    "valid_periods = np.load(valid_periods_path)\n",
    "training_data = np.load(training_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for sequence and forecast lengths\n",
    "seq_length = 128\n",
    "forecast_length = 9\n",
    "\n",
    "# Initialize two scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess data\n",
    "def preprocess_data(data, valid_periods, seq_length, forecast_length, scaler_X, scaler_y):\n",
    "    X, y = [], []\n",
    "    for i, row in enumerate(data):\n",
    "        valid_data = row[valid_periods[i][0]:valid_periods[i][1]]\n",
    "        if valid_data.size > 0:\n",
    "            for j in range(len(valid_data) - seq_length - forecast_length + 1):\n",
    "                seq_X = valid_data[j:(j + seq_length)]\n",
    "                seq_y = valid_data[(j + seq_length):(j + seq_length + forecast_length)]\n",
    "                \n",
    "                X.append(seq_X)\n",
    "                y.append(seq_y)\n",
    "    \n",
    "    X = scaler_X.fit_transform(np.array(X))\n",
    "    y = scaler_y.fit_transform(np.array(y))\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "X, y = preprocess_data(training_data, valid_periods, seq_length, forecast_length, scaler_X, scaler_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fitted scalers\n",
    "scaler_X_filename = \"/mnt/data/scaler_X.save\"\n",
    "scaler_y_filename = \"/mnt/data/scaler_y.save\"\n",
    "dump(scaler_X, scaler_X_filename)\n",
    "dump(scaler_y, scaler_y_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "val_size = int(len(X) * 0.2)\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "X_train, y_train = X[:-val_size], y[:-val_size]\n",
    "X_val, y_val = X[-val_size:], y[-val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data for LSTM input\n",
    "X_train = X_train.reshape((X_train.shape[0], seq_length, 1))\n",
    "y_train = y_train.reshape((y_train.shape[0], forecast_length, 1))\n",
    "X_val = X_val.reshape((X_val.shape[0], seq_length, 1))\n",
    "y_val = y_val.reshape((y_val.shape[0], forecast_length, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model building function\n",
    "def build_model(input_shape, lstm_units, dropout_rate, forecast_length):\n",
    "    input_layer = tfkl.Input(shape=input_shape)\n",
    "    x = tfkl.Bidirectional(tfkl.LSTM(lstm_units, return_sequences=True))(input_layer)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(dropout_rate)(x)\n",
    "    x = tfkl.LSTM(lstm_units // 2)(x)\n",
    "    x = tfkl.BatchNormalization()(x)\n",
    "    x = tfkl.Dropout(dropout_rate)(x)\n",
    "    output_layer = tfkl.Dense(forecast_length, activation='linear')(x)  # Linear activation for regression\n",
    "\n",
    "    model = tfk.Model(inputs=input_layer, outputs=output_layer)\n",
    "    return model\n",
    "\n",
    "# Model configuration\n",
    "input_shape = (seq_length, 1)\n",
    "dropout_rate = 0.2\n",
    "lstm_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "model = build_model(input_shape, lstm_units, dropout_rate, forecast_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "model.compile(optimizer=optimizer, loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "early_stopping = tfk.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation set (or test set if available)\n",
    "evaluation_results = model.evaluate(X_val, y_val)\n",
    "print(f'Validation Loss: {evaluation_results}')\n",
    "\n",
    "# Save the model\n",
    "model_save_path = '/mnt/data/my_lstm_model'\n",
    "model.save(model_save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
